#!/bin/bash -ex
set -o pipefail

#
# Run tests with OSU
#

# Host buffers
unset MPICH_GPU_SUPPORT_ENABLED
rm -rf run.sh 
cat > run.sh << EOF
#!/bin/bash -e
cd /myrun

set -x
\$OSU_PATH/libexec/osu-micro-benchmarks/mpi/collective/osu_bcast
EOF
chmod +x run.sh 

$SCMD \
    -B $(pwd):/myrun \
    $1 \
    /myrun/run.sh |& tee res.log


# Host buffers with device enabled
sleep 5
export MPICH_GPU_SUPPORT_ENABLED=1
rm -rf run.sh 
cat > run.sh << EOF
#!/bin/bash -e
cd /myrun

set -x
\$OSU_PATH/libexec/osu-micro-benchmarks/mpi/collective/osu_bcast
EOF
chmod +x run.sh 

$SCMD \
    -B $(pwd):/myrun \
    $1 \
    /myrun/run.sh |& tee res.log

# Device  buffers with device enabled
sleep 5
export MPICH_GPU_SUPPORT_ENABLED=1
rm -rf run.sh 
cat > run.sh << EOF
#!/bin/bash -e
cd /myrun

set -x
\$OSU_PATH/libexec/osu-micro-benchmarks/mpi/collective/osu_bcast -d rocm
EOF
chmod +x run.sh 

$SCMD \
    -B $(pwd):/myrun \
    $1 \
    /myrun/run.sh |& tee res.log

#
# Example of mpi4py with CUDA arrays test from MPI4PY examples.
#
cat > use_cupy.py << EOF
# Demonstrate how to work with Python GPU arrays using CUDA-aware MPI.
# We choose the CuPy library for simplicity, but any CUDA array which
# has the __cuda_array_interface__ attribute defined will work.

from mpi4py import MPI
import cupy

comm = MPI.COMM_WORLD
size = comm.Get_size()
rank = comm.Get_rank()

# Allreduce
if rank == 0:
  print("Starting allreduce test...")
sendbuf = cupy.arange(10, dtype='i')
recvbuf = cupy.empty_like(sendbuf)
# always make sure the GPU buffer is ready before any MPI operation
cupy.cuda.get_current_stream().synchronize()
comm.Allreduce(sendbuf, recvbuf)
assert cupy.allclose(recvbuf, sendbuf*size)
if rank == 0:
  print("Done!")

# Bcast
if rank == 0:
  print("Starting bcast test...")
if rank == 0:
    buf = cupy.arange(100, dtype=cupy.complex64)
else:
    buf = cupy.empty(100, dtype=cupy.complex64)
cupy.cuda.get_current_stream().synchronize()
comm.Bcast(buf)
assert cupy.allclose(buf, cupy.arange(100, dtype=cupy.complex64))
if rank == 0:
  print("Done!")

# Send-Recv
if rank == 0:
  print("Starting send-recv test...")
  
if rank == 0:
    buf = cupy.arange(20, dtype=cupy.float64)
    cupy.cuda.get_current_stream().synchronize()
    for j in range(1,size):
        comm.Send(buf, dest=j, tag=88+j)
else:
    buf = cupy.empty(20, dtype=cupy.float64)
    cupy.cuda.get_current_stream().synchronize()
    comm.Recv(buf, source=0, tag=88+rank)
    assert cupy.allclose(buf, cupy.arange(20, dtype=cupy.float64))
    
if rank == 0:
  print("Done!")
EOF

sleep 5
export MPICH_GPU_SUPPORT_ENABLED=1
rm -rf run.sh 
cat > run.sh << EOF
#!/bin/bash -e
cd /myrun

set -x
python -u use_cupy.py
EOF
chmod +x run.sh 

$SCMD \
    -B $(pwd):/myrun \
    $1 \
    /myrun/run.sh |& tee res.log
