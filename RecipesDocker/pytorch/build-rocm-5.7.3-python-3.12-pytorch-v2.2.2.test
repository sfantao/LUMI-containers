#!/bin/bash -ex
set -o pipefail

#
# Run RCCL tests
#

rm -rf run.sh 
cat > run.sh << EOF
#!/bin/bash -e
cd /myrun
set -x

# -b minbytes
# -e maxbytes
# -f increment factor
# -g gpus per thread
/opt/rccltests/all_reduce_perf -z 1 -b 2M -e 2048M -f 2 -g 1 -t 1 -R 1 -n 80 -w 5 -d half

EOF
chmod +x run.sh 

$SCMD \
    -B $(pwd):/myrun \
    $1 \
    /myrun/run.sh |& tee res.log

# Check BW for large transfers which are more stable
for i in $(grep -r 'half     sum' res.log | tail -n5 | awk '{print $8}') ; do
  echo "Measure RCCL test all-reduce BW to be $i..."
  if (( $(echo "$i < 60.00" |bc -l) )); then
    echo "Too low!!!";
    exit 1
  fi
done

#
# Example of mpi4py with CUDA arrays test from MPI4PY examples.
#
cat > use_cupy.py << EOF
# Demonstrate how to work with Python GPU arrays using CUDA-aware MPI.
# We choose the CuPy library for simplicity, but any CUDA array which
# has the __cuda_array_interface__ attribute defined will work.

from mpi4py import MPI
import cupy

comm = MPI.COMM_WORLD
size = comm.Get_size()
rank = comm.Get_rank()

# Allreduce
if rank == 0:
  print("Starting allreduce test...")
sendbuf = cupy.arange(10, dtype='i')
recvbuf = cupy.empty_like(sendbuf)
# always make sure the GPU buffer is ready before any MPI operation
cupy.cuda.get_current_stream().synchronize()
comm.Allreduce(sendbuf, recvbuf)
assert cupy.allclose(recvbuf, sendbuf*size)
if rank == 0:
  print("Done!")

# Bcast
if rank == 0:
  print("Starting bcast test...")
if rank == 0:
    buf = cupy.arange(100, dtype=cupy.complex64)
else:
    buf = cupy.empty(100, dtype=cupy.complex64)
cupy.cuda.get_current_stream().synchronize()
comm.Bcast(buf)
assert cupy.allclose(buf, cupy.arange(100, dtype=cupy.complex64))
if rank == 0:
  print("Done!")

# Send-Recv
if rank == 0:
  print("Starting send-recv test...")
  
if rank == 0:
    buf = cupy.arange(20, dtype=cupy.float64)
    cupy.cuda.get_current_stream().synchronize()
    for j in range(1,size):
        comm.Send(buf, dest=j, tag=88+j)
else:
    buf = cupy.empty(20, dtype=cupy.float64)
    cupy.cuda.get_current_stream().synchronize()
    comm.Recv(buf, source=0, tag=88+rank)
    assert cupy.allclose(buf, cupy.arange(20, dtype=cupy.float64))
    
if rank == 0:
  print("Done!")
EOF

rm -rf run.sh 
cat > run.sh << EOF
#!/bin/bash -e
cd /myrun

set -x

ROCM_MAJOR_RELEASE=\$(echo \$ROCM_RELEASE | cut -d. -f1)
ROCM_MINOR_RELEASE=\$(echo \$ROCM_RELEASE | cut -d. -f2)

if [ \$ROCM_MAJOR_RELEASE -lt 6 ] ; then
  exit 0
fi
if [ \$ROCM_MINOR_RELEASE -ge 2 ] ; then
  exit 0
fi

if python -c 'import mpi4py ; print(mpi4py.__version__)' ; then
  ROCR_VISIBLE_DEVICE=\$SLURM_LOCALID python -u use_cupy.py
fi

EOF
chmod +x run.sh 

MPICH_GPU_SUPPORT_ENABLED=1 \
$SCMD \
    -B $(pwd):/myrun \
    $1 \
    /myrun/run.sh |& tee res.log

#
# Run MNIST test
#
cat > mnist.py << EOF
import os
from datetime import datetime
import argparse
import torch.multiprocessing as mp
import torchvision
import torchvision.transforms as transforms
import torch
import torch.nn as nn
import torch.distributed as dist
from apex.parallel import DistributedDataParallel as DDP
from apex import amp


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('-lrank', '--local_rank', default=0, type=int,
                        help='ranking within the nodes')
    parser.add_argument('--epochs', default=2, type=int, metavar='N',
                        help='number of total epochs to run')
                        
    args = parser.parse_args()
    args.world_size = int(os.environ['WORLD_SIZE'])
    args.rank = int(os.environ['RANK'])
    train(args)


class ConvNet(nn.Module):
    def __init__(self, num_classes=10):
        super(ConvNet, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, 16, kernel_size=5, stride=1, padding=2),
            nn.BatchNorm2d(16),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),
            nn.BatchNorm2d(32),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.fc = nn.Linear(7*7*32, num_classes)

    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc(out)
        return out


def train( args):
    rank = args.rank
    gpu = args.local_rank
    dist.init_process_group(backend='nccl', init_method='env://', world_size=args.world_size, rank=rank)
    torch.manual_seed(0)
    model = ConvNet()
    torch.cuda.set_device(gpu)
    model.cuda(gpu)
    batch_size = 8
    # define loss function (criterion) and optimizer
    criterion = nn.CrossEntropyLoss().cuda(gpu)
    optimizer = torch.optim.SGD(model.parameters(), 1e-4)
    # Wrap the model
    model = nn.parallel.DistributedDataParallel(model, device_ids=[gpu])
    # Data loading code
    train_dataset = torchvision.datasets.MNIST(root='/datasets',
                                               train=True,
                                               transform=transforms.ToTensor(),
                                               download=False)
    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset,
                                                                    num_replicas=args.world_size,
                                                                    rank=rank)
    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                               batch_size=batch_size,
                                               shuffle=False,
                                               num_workers=0,
                                               pin_memory=True,
                                               sampler=train_sampler)

    start = datetime.now()
    total_step = len(train_loader)
    for epoch in range(args.epochs):
        for i, (images, labels) in enumerate(train_loader):
            images = images.cuda(non_blocking=True)
            labels = labels.cuda(non_blocking=True)
            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            if (i + 1) % 10 == 0 and rank == 0:
                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch + 1, args.epochs, i + 1, total_step,
                                                                         loss.item()))
    if rank == 0:
        print("Training complete in: " + str(datetime.now() - start))


if __name__ == '__main__':
    main()
EOF
cat > mnist-download.py << EOF
import torchvision
import torchvision.transforms as transforms
torchvision.datasets.MNIST(root='/datasets',
                           train=True,
                           transform=transforms.ToTensor(),
                           download=True)
EOF
cat > get-master.py << EOF
import argparse
def get_parser():
    parser = argparse.ArgumentParser(description="Extract master node name from Slurm node list",
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("nodelist", help="Slurm nodelist")
    return parser


if __name__ == '__main__':
    parser = get_parser()
    args = parser.parse_args()

    first_nodelist = args.nodelist.split(',')[0]

    if '[' in first_nodelist:
        a = first_nodelist.split('[')
        first_node = a[0] + a[1].split('-')[0]

    else:
        first_node = first_nodelist

    print(first_node)
EOF

rm -rf run.sh 
cat > run.sh << EOF
#!/bin/bash -e
cd /myrun
set -x

  if ! python -c 'import apex' ; then
    exit 0
  fi

  # Download on single rank
  # if [ \$SLURM_PROCID -eq 0 ] ; then
  #   python mnist-download.py
  # fi
  # exit 0
  
  if [[ \$ROCM_RELEASE != "6.2."* ]] ; then
    export MIOPEN_USER_DB_PATH="/tmp/$(whoami)-miopen-cache-\$SLURM_NODEID"
    export MIOPEN_CUSTOM_CACHE_DIR=\$MIOPEN_USER_DB_PATH

    # Set MIOpen cache out of the home folder.
    if [ \$SLURM_LOCALID -eq 0 ] ; then
      rm -rf \$MIOPEN_USER_DB_PATH
      mkdir -p \$MIOPEN_USER_DB_PATH
    fi
    sleep 3
  fi
  
  # Report affinity
  echo "Rank \$SLURM_PROCID --> \$(taskset -p \$\$)"
  
  # Set master the first node of the allocation. Also select some port to use and leverage
  # SLURM environment to specify ranks to pytorch DDP.
  export MASTER_ADDR=\$(python get-master.py "\$SLURM_NODELIST")
  export MASTER_PORT=29500
  export WORLD_SIZE=\$SLURM_NPROCS
  export RANK=\$SLURM_PROCID
  
  echo "--> MASTER_ADDR: \$MASTER_ADDR"
  echo "--> MASTER_PORT: \$MASTER_PORT"
  echo "--> WORLD_SIZE: \$WORLD_SIZE"
  echo "--> RANK: \$RANK" 
  
  python -u mnist.py \
    --local_rank \$SLURM_LOCALID \
    --epochs 10
EOF
chmod +x run.sh 

$SCMD \
    -B $(pwd):/myrun \
    -B /pfs/lustrep4/scratch/project_462000475/containers-ci/data-sets/mnist:/datasets \
    $1 \
    /myrun/run.sh |& tee res.log

#
# Test deepspeed
#

rm -rf run.sh 
cat > run.sh << EOF
#!/bin/bash -e
cd /myrun
set -x

  # Single node
  if [ \$SLURM_LOCALID -ne 0 ] ; then
    exit 0
  fi
  
  if which ds_report &> /dev/null ; then
    ds_report
  fi
EOF
chmod +x run.sh 

$SCMD \
    -B $(pwd):/myrun \
    $1 \
    /myrun/run.sh |& tee res.log

#
# Test flash attention
#

rm -rf run.sh
cat > run.sh << EOF
#!/bin/bash -e
cd /myrun
set -x

  # Single node
  if [ \$SLURM_LOCALID -ne 0 ] ; then
    exit 0
  fi

  # Disable xformers to make sure they don't interfere with flash attention testing.
  if [ -f /opt/wheels/flash_attn-benchmarks/benchmark_flash_attention.py ] ; then
    sed 's#import xformers.ops as xops#xops = None#g' /opt/wheels/flash_attn-benchmarks/benchmark_flash_attention.py > benchmark_flash_attention.py
    python -u benchmark_flash_attention.py
  fi
EOF
chmod +x run.sh

$SCMD \
    -B $(pwd):/myrun \
    $1 \
    /myrun/run.sh |& tee res.log

#
# Test xformers
#

rm -rf run.sh
cat > run.sh << EOF
#!/bin/bash -e
cd /myrun
set -x

  # Single node
  if [ \$SLURM_LOCALID -ne 0 ] ; then
    exit 0
  fi

  # Disable xformers to make sure they don't interfere with flash attention testing.
  if python -c 'import xformers ; print(xformers.__file__)' ; then
    script="\$(dirname \$(python -c 'import xformers ; print(xformers.__file__)'))/benchmarks/benchmark_mem_eff_attention.py"
    # This is a bit flaky because of triton JIT that randomly fails to find the executable.
    python -u \$script || true
  fi
EOF
chmod +x run.sh

$SCMD \
    -B $(pwd):/myrun \
    $1 \
    /myrun/run.sh |& tee res.log

#
# Test vLLM
#
cat > vllm_testcase.py << EOF
from vllm import LLM, SamplingParams

def work():
  # Sample prompts.
  prompts = [
      "Hello, my name is",
      "The president of the United States is",
      "The capital of France is",
      "The future of AI is",
  ]
  # Create a sampling params object.
  sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

  # Create an LLM.
  llm = LLM(model="facebook/opt-125m", download_dir="/myrun/vllm-download-dir")
  # Generate texts from the prompts. The output is a list of RequestOutput objects
  # that contain the prompt, generated text, and other information.
  outputs = llm.generate(prompts, sampling_params)
  # Print the outputs.
  for output in outputs:
      prompt = output.prompt
      generated_text = output.outputs[0].text
      print(f"Prompt: {prompt!r}, Generated text: {generated_text!r}")

if __name__ == '__main__':
  work()
    
EOF

rm -rf run.sh
cat > run.sh << EOF
#!/bin/bash -e
cd /myrun
set -x

  export VLLM_CACHE_ROOT=/tmp/my-vllm-cache-\$SLURM_LOCALID
  mkdir -p \$VLLM_CACHE_ROOT

  export HF_HOME=/hf_home

  export AITER_JIT_DIR=/tmp/my-aiter-jit-dir-\$SLURM_LOCALID
  mkdir -p \$AITER_JIT_DIR/build
  export CC=clang
  export CXX=clang++

  # Disable xformers to make sure they don't interfere with flash attention testing.
  if python -c 'import vllm ; print(vllm.__version__)' ; then
    HIP_VISIBLE_DEVICES=\$SLURM_LOCALID python -u vllm_testcase.py
  fi
EOF
chmod +x run.sh

$SCMD \
    -B /pfs/lustrep4/scratch/project_462000475/containers-ci/data-sets/hf_home:/hf_home \
    -B $(pwd):/myrun \
    $1 \
    /myrun/run.sh |& tee res.log

#
# Test bits and bytes
#
cat > bitsandbytes_test.py << EOF
import torch
from transformers import LlamaForCausalLM, LlamaTokenizer

MAX_NEW_TOKENS = 128
model_name = "meta-llama/Llama-2-7b-hf"

text = "Hamburg is in which country?\n"
tokenizer = LlamaTokenizer.from_pretrained(model_name)
input_ids = tokenizer(text, return_tensors="pt").input_ids

max_memory = f"{int(torch.cuda.mem_get_info()[0]/1024**3)-2}GB"

n_gpus = torch.cuda.device_count()
max_memory = {i: max_memory for i in range(n_gpus)}

model = LlamaForCausalLM.from_pretrained(model_name, device_map="auto", load_in_8bit=True, max_memory=max_memory)

generated_ids = model.generate(input_ids, max_length=MAX_NEW_TOKENS)
print(tokenizer.decode(generated_ids[0], skip_special_tokens=True))
EOF

rm -rf run.sh
cat > run.sh << EOF
#!/bin/bash -e
cd /myrun
set -x
  
  export HF_HOME=/hf_home

  if python -c 'import bitsandbytes ; print(bitsandbytes.__version__)' ; then
    HIP_VISIBLE_DEVICES=\$SLURM_LOCALID python -u bitsandbytes_test.py
  fi
EOF
chmod +x run.sh

$SCMD \
    -B $(pwd):/myrun \
    -B /pfs/lustrep4/scratch/project_462000475/containers-ci/data-sets/hf_home:/hf_home \
    $1 \
    /myrun/run.sh |& tee res.log

#
# Tests transformer engine
#

rm -rf run.sh
cat > run.sh << EOF
#!/bin/bash -e
set -x

  export HF_HOME=/hf_home

  if python -c 'import transformer_engine ; print(transformer_engine.__version__)' ; then

    cd /mnist
    HIP_VISIBLE_DEVICES=\$SLURM_LOCALID python mnist-te.py --use-te  --use-amp --epochs 3

    #cd /minGPT
    #HIP_VISIBLE_DEVICES=\$SLURM_LOCALID python -u gptSort.py --use-te  --use-amp --miters 320

  fi
EOF
chmod +x run.sh

$SCMD \
    -B $(pwd):/myrun \
    -B /pfs/lustrep4/scratch/project_462000475/containers-ci/data-sets/mnist:/datasets \
    -B /pfs/lustrep4/scratch/project_462000475/containers-ci/data-sets/TransformerEngine/examples/pytorch/mnist:/mnist \
    -B /pfs/lustrep4/scratch/project_462000475/containers-ci/data-sets/TransformerEngine/examples/pytorch/minGPT:/minGPT \
    $1 \
    /myrun/run.sh |& tee res.log

#
# Megatron
#

rm -rf run.sh
cat > run.sh << EOF
#!/bin/bash -e

if python -c 'import megatron' ; then
  echo "Testing..."
else
  exit 0
fi

# For AITER:
export JIT_WORKSPACE_DIR=/tmp/my-aiter-jit-workspace-\$SLURM_LOCALID/private_aiter/jit
rm -rf \$JIT_WORKSPACE_DIR
mkdir -p \$JIT_WORKSPACE_DIR/build
export PYTHONPATH=/tmp/my-aiter-jit-workspace-\$SLURM_LOCALID
export GPU_ARCHS=gfx90a

cd /megatron-code/examples/gpt3

set -x

# Runs the "175B" parameter model

export MASTER_PORT=29500
export WORLD_SIZE=\$SLURM_NPROCS
export RANK=\$SLURM_PROCID
export LOCAL_RANK=\$SLURM_LOCALID

export CUDA_DEVICE_MAX_CONNECTIONS=1
export CC=gcc-12
export CXX=g++-12

export NVTE_FRAMEWORK=pytorch
export NVTE_ROCM_ARCH=gfx90a

CHECKPOINT_PATH=/myrun #<Specify path>
TENSORBOARD_LOGS_PATH=/myrun #<Specify path>
VOCAB_FILE=/megatron-data/gpt2-vocab.json #<Specify path to file>/gpt2-vocab.json
MERGE_FILE=/megatron-data/gpt2-merges.txt #<Specify path to file>/gpt2-merges.txt
DATA_PATH=/megatron-data/my-gpt2_text_document #<Specify path and file prefix>_text_document

GPT_MODEL_ARGS=(
    --num-layers 96 
    --hidden-size 12288 
    --num-attention-heads 96 
    --seq-length 2048 
    --max-position-embeddings 2048 
    --attention-backend flash # Can use (flash/fused/unfused/local)
)

TRAINING_ARGS=(
    --micro-batch-size 8
    --global-batch-size 2048 
    # sfantao --rampup-batch-size 16 16 5859375 
    --train-iters 500000 
    --weight-decay 0.1 
    --adam-beta1 0.9 
    --adam-beta2 0.95 
    --init-method-std 0.006 
    --clip-grad 1.0 
    --fp16
    --lr 6.0e-5 
    --lr-decay-style cosine 
    --min-lr 6.0e-6
    --lr-warmup-fraction .001 
    --lr-decay-iters 430000 
)

MODEL_PARALLEL_ARGS=(
	--tensor-model-parallel-size 1 #sfantao 8 
	--pipeline-model-parallel-size 1 #sfantao 16 
)

DATA_ARGS=(
    --data-path \$DATA_PATH 
    --vocab-file \$VOCAB_FILE 
    --merge-file \$MERGE_FILE 
    --split 949,50,1
)

EVAL_AND_LOGGING_ARGS=(
    --log-interval 1
    --log-throughput
    --log-progress
    --save-interval 10000 
    --eval-interval 1000 
    --eval-iters 3
    --tensorboard-dir \$TENSORBOARD_LOGS_PATH 
)

EXTRA_ARGS=(
       --num-layers 32
       --hidden-size 4096
       --num-attention-heads 32
       --seq-length 1024
       --tensor-model-parallel-size 2
       --pipeline-model-parallel-size 2
       --sequence-parallel
       --distributed-timeout-minutes 3
       --no-gradient-accumulation-fusion
       --num-workers 4
    #    --profile-ranks 6
    #    --profile-step-start 31
    #    --profile-step-end 34
    #    --use-pytorch-profiler
    #    --profile
       --train-iters 5
)

export NCCL_NCHANNELS_PER_PEER=4

python -u /megatron-code/pretrain_gpt.py \
    \${GPT_MODEL_ARGS[@]} \
    \${TRAINING_ARGS[@]} \
    \${MODEL_PARALLEL_ARGS[@]} \
    \${DATA_ARGS[@]} \
    \${EVAL_AND_LOGGING_ARGS[@]} \
    \${EXTRA_ARGS[@]}

EOF
chmod +x run.sh

#     -B /pfs/lustrep4/scratch/project_462000475/containers-ci/data-sets/megatron-fe353fd:/megatron-code 
#     -B /pfs/lustrep4/scratch/project_462000475/containers-ci/data-sets/megatron-856c36d:/megatron-code

MASTER_ADDR=$(scontrol show hostname "$SLURM_NODELIST" | head -n1) \
$SCMD \
    -B /boot/config-5.14.21-150500.55.49_13.0.56-cray_shasta_c \
    -B $(pwd):/myrun \
    -B /pfs/lustrep4/scratch/project_462000475/containers-ci/data-sets/gpt2:/megatron-data \
    -B /pfs/lustrep4/scratch/project_462000475/containers-ci/data-sets/megatron-fe353fd:/megatron-code \
    $1 \
    /myrun/run.sh |& tee res.log
