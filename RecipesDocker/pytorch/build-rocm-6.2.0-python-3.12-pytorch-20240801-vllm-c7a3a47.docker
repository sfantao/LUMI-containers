#
# Install conda environment
# 
ARG PYTHON_VERSION
RUN $WITH_CONDA; set -eux ; \
  conda create -n pytorch python=$PYTHON_VERSION ; \
  conda activate pytorch ; \
  conda install -y ninja pillow cmake pyyaml

# Don't use old libstdc++ that comes with conda.
RUN $WITH_CONDA; set -eux ; \
  rm /opt/miniconda3/envs/pytorch/lib/libstdc++.so.*

ENV WITH_CONDA "source /opt/miniconda3/bin/activate pytorch"

# Repository for the wheel files
RUN set -eux ; \
  mkdir /opt/wheels

# Build LLVM so that we can build aotriton.
# The LLVM hash comes from /opt/mybuild/build/aotriton/src/third_party/triton/llvm-hash.txt
# In this case: 49af6502c6dcb4a7f7520178bd14df396f78240c.
RUN $WITH_CONDA; set -eux ; \
  git clone --recursive https://github.com/llvm/llvm-project /opt/llvm-src ; \
  cd /opt/llvm-src ; \
  git checkout -b mydev 49af6502c6dcb4a7f7520178bd14df396f78240c ; \
  git submodule sync ; \
  git submodule update --init --recursive --jobs 0 ; \
  mkdir /opt/llvm-src/build ; \
  cd /opt/llvm-src/build ; \
  cmake -G Ninja -DCMAKE_BUILD_TYPE=Release -DLLVM_ENABLE_ASSERTIONS=ON /opt/llvm-src/llvm -DLLVM_ENABLE_PROJECTS="mlir;llvm" -DLLVM_TARGETS_TO_BUILD="host;NVPTX;AMDGPU" -DCMAKE_INSTALL_PREFIX=/opt/llvm ; \
  ninja ; \
  ninja install ; \
  rm -rf /opt/llvm-src ; \
  true

ENV LLVM_INCLUDE_DIRS /opt/llvm/include
ENV LLVM_LIBRARY_DIR /opt/llvm/lib
ENV LLVM_SYSPATH /opt/llvm

#
# Install pytorch
# 

ENV PYTORCH_ROCM_ARCH gfx90a 
ARG PYTORCH_VERSION
ARG PYTORCH_DEBUG
ARG PYTORCH_RELWITHDEBINFO

RUN $WITH_CONDA; set -eux ; \
  git clone --recursive -b nightly https://github.com/pytorch/pytorch /opt/mybuild ; \
  cd /opt/mybuild ; \
  git checkout -b mydev $PYTORCH_VERSION ; \
  git submodule sync ; \
  git submodule update --init --recursive --jobs 0 ; \
  \
  pip install -r /opt/mybuild/requirements.txt ; \
  \
  cd /opt/mybuild ; \
  \
  nice python3 tools/amd_build/build_amd.py ; \
  sed -i 's#-Werror##g' third_party/fbgemm/CMakeLists.txt ; \
  sed -i '/-Werror=format/a append_cxx_flag_if_supported("-Wno-error=nonnull" CMAKE_CXX_FLAGS)' CMakeLists.txt ; \
  \
  CMAKE_PREFIX_PATH=$CONDA_PREFIX \
    DEBUG=$PYTORCH_DEBUG \
    REL_WITH_DEB_INFO=$PYTORCH_RELWITHDEBINFO \
    CC=gcc-12 \
    CXX=g++-12 \
    PYTORCH_ROCM_ARCH="gfx90a" \
    VERBOSE=1 \
    V=1 \
    USE_ROCM=1 \
    PROCS=48 \
    python3 setup.py bdist_wheel ; \
  \
  mv dist/torch-*.whl /opt/wheels ; \
  cd /opt ; rm -rf /opt/mybuild

RUN $WITH_CONDA; set -eux ; \
  pip install /opt/wheels/torch-*.whl

#
# Torchvision
#

ARG TORCHVISION_VERSION
RUN $WITH_CONDA; set -eux ; \
  git clone --recursive -b nightly https://github.com/pytorch/vision /opt/mybuild ; \
  cd /opt/mybuild ; \
  git checkout -b mydev $TORCHVISION_VERSION ; \
  git submodule sync ; \
  git submodule update --init --recursive --jobs 0 ; \
  true

# Use system libtinfo to avoid version warnings.
RUN set -eux ; \
  find /opt -iname libtinfo.* | xargs rm -rf

# Make sure TBB is on - it is needed by libtorch.
RUN $WITH_CONDA; set -eux ; \
  conda install -y tbb

ENV LD_LIBRARY_PATH $LD_LIBRARY_PATH:/opt/miniconda3/envs/pytorch/lib

# Install, letting torchvision build figure out the existing torch version.
RUN $WITH_CONDA; set -ux ; \
  cd /opt/mybuild ; \
  unset PYTORCH_VERSION ; \
  CC=gcc-12 CXX=g++-12 FORCE_CUDA=1 python3 setup.py bdist_wheel --dist-dir=/opt/wheels ; \
  \
  cd /opt ; rm -rf /opt/mybuild

RUN $WITH_CONDA; set -eux ; \
  pip install /opt/wheels/torchvision-*.whl

#
# AMD-SMI
#
RUN $WITH_CONDA; set -eux ; \
  cd $ROCM_PATH/share/amd_smi ; \
  python3 -m pip wheel . --wheel-dir=/opt/wheels

#
# Flash-Attention
#
RUN $WITH_CONDA; set -eux ; \
  git clone https://github.com/ROCm/flash-attention.git /opt/mybuild ; \
  cd /opt/mybuild ; \
  git checkout 23a2b1c ; \
  git submodule sync ; \
  git submodule update --init --recursive --jobs 0 ; \
  MAX_JOBS=32 \
    GPU_ARCHS=gfx90a \
    CC=gcc-12 \
    CXX=g++-12 \
    python3 setup.py bdist_wheel --dist-dir=/opt/wheels ; \
  cd / ; rm -rf /opt/mybuild 

#
# Triton wheel build stage
#
RUN $WITH_CONDA; set -eux ; \
  git clone https://github.com/OpenAI/triton.git /opt/mybuild ; \
  cd /opt/mybuild ; \
  git checkout e0fc12c ; \
  cd /opt/mybuild/python ; \
  \
  unset LLVM_INCLUDE_DIRS ; \
  unset LLVM_LIBRARY_DIR ; \
  unset LLVM_SYSPATH ; \
  \
  CC=gcc-12 \
    CXX=g++-12 \
    python3 setup.py bdist_wheel --dist-dir=/opt/wheels ; \
  cd / ; rm -rf /opt/mybuild 

#
# vLLM
#
RUN $WITH_CONDA; set -eux ; \
  python3 -m pip install --upgrade numba scipy huggingface-hub[cli]

# Workaround for ray >= 2.10.0
ENV RAY_EXPERIMENTAL_NOSET_ROCR_VISIBLE_DEVICES=1

# Silences the HF Tokenizers warning
ENV TOKENIZERS_PARALLELISM=false

# Make sure punica kernels are built (for LoRA)
ENV VLLM_INSTALL_PUNICA_KERNELS=1

# Performance environment variable.
ENV HIP_FORCE_DEV_KERNARG=1

# Using the ROCm fork and enabling python 3.12
RUN $WITH_CONDA; set -eux ; \
# git clone https://github.com/vllm-project/vllm /opt/mybuild ; \
  git clone https://github.com/rocm/vllm /opt/mybuild ; \
  cd /opt/mybuild ; \
# git checkout -b mydev baaedfd ; \
  git checkout -b mydev c7a3a47 ; \
  \
  git submodule sync ; \
  git submodule update --init --recursive --jobs 0 ; \
  \
  sed -i 's#3.11#3.12#g' CMakeLists.txt ; \
  \
  python3 -m pip install -r requirements-rocm.txt ; \
  \
  CC=gcc-12 \
    CXX=g++-12 \
    python3 setup.py bdist_wheel --dist-dir=/opt/wheels ; \
  \
  cd /opt/mybuild/gradlib ; \
  CC=gcc-12 \
    CXX=g++-12 \
    python3 setup.py bdist_wheel --dist-dir=/opt/wheels ; \
  \
  rm -rf /opt/mybuild

RUN $WITH_CONDA; set -eux ; \
  pip install \
    /opt/wheels/amdsmi-*.whl \
    /opt/wheels/flash_attn-*.whl \
    /opt/wheels/triton-*.whl \
    /opt/wheels/gradlib-*.whl \
    /opt/wheels/vllm-*.whl
